{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1v7OeDcPT2anuj61NNckQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"P1Cey116wiTQ","executionInfo":{"status":"ok","timestamp":1766401977872,"user_tz":-330,"elapsed":16186,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark=SparkSession.builder\\\n","    .appName(\"Day20\")\\\n","    .getOrCreate()"]},{"cell_type":"code","source":["raw_drivers = [\n","(\"D001\",\"Ramesh\",\"35\",\"Hyderabad\",\"Car,Bike\"),\n","(\"D002\",\"Suresh\",\"Forty\",\"Bangalore\",\"Auto\"),\n","(\"D003\",\"Anita\",None,\"Mumbai\",[\"Car\"]),\n","(\"D004\",\"Kiran\",\"29\",\"Delhi\",\"Car|Bike\"),\n","(\"D005\",\"\", \"42\",\"Chennai\",None)\n","]\n","raw_cities = [\n","(\"Hyderabad\",\"South\"),\n","(\"Bangalore\",\"South\"),\n","(\"Mumbai\",\"West\"),\n","(\"Delhi\",\"North\"),\n","(\"Chennai\",\"South\")\n","]\n","raw_trips = [\n","(\"T001\",\"D001\",\"Hyderabad\",\"2024-01-05\",\"Completed\",\"450\"),\n","(\"T002\",\"D002\",\"Bangalore\",\"05/01/2024\",\"Cancelled\",\"0\"),\n","(\"T003\",\"D003\",\"Mumbai\",\"2024/01/06\",\"Completed\",\"620\"),\n","(\"T004\",\"D004\",\"Delhi\",\"invalid_date\",\"Completed\",\"540\"),\n","(\"T005\",\"D001\",\"Hyderabad\",\"2024-01-10\",\"Completed\",\"700\"),\n","(\"T006\",\"D005\",\"Chennai\",\"2024-01-12\",\"Completed\",\"350\")\n","]\n","raw_activity = [\n","(\"D001\",\"login,accept_trip,logout\",\"{'device':'mobile'}\",180),\n","(\"D002\",[\"login\",\"logout\"],\"device=laptop\",60),\n","(\"D003\",\"login|accept_trip\",None,120),\n","(\"D004\",None,\"{'device':'tablet'}\",90),\n","(\"D005\",\"login\",\"{'device':'mobile'}\",30)\n","]\n"],"metadata":{"id":"wkfU0JxHwvQ0","executionInfo":{"status":"ok","timestamp":1766403209185,"user_tz":-330,"elapsed":22,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType,DoubleType\n","drivers_schema=StructType([\n","    StructField(\"driver_id\",StringType(),True),\n","    StructField(\"name\",StringType(),True),\n","    StructField(\"age\",StringType(),True),\n","    StructField(\"city\",StringType(),True),\n","    StructField(\"vehicle_types\",StringType(),True)\n","])\n","\n","cities_schema=StructType([\n","    StructField(\"city\",StringType(),True),\n","    StructField(\"region\",StringType(),True)\n","])\n","trips_schema=StructType([\n","    StructField(\"trip_id\",StringType(),True),\n","    StructField(\"driver_id\",StringType(),True),\n","    StructField(\"city\",StringType(),True),\n","    StructField(\"trip_date_raw\",StringType(),True),\n","    StructField(\"trip_status\",StringType(),True),\n","    StructField(\"fare_raw\",StringType(),True)\n","])\n","activity_schema=StructType([\n","    StructField(\"driver_id\",StringType(),True),\n","    StructField(\"actions\",StringType(),True),\n","    StructField(\"metadata_raw\", StringType(), True),\n","    StructField(\"session_duration_sec\", IntegerType(), True),\n","\n","])\n","drivers_df0 = spark.createDataFrame(raw_drivers, schema=drivers_schema)\n","cities_df   = spark.createDataFrame(raw_cities,  schema=cities_schema)\n","trips_df0   = spark.createDataFrame(raw_trips,   schema=trips_schema)\n","activity_df0= spark.createDataFrame(raw_activity,schema=activity_schema)\n","\n","drivers_df0.show(truncate=False);\n","trips_df0.show(truncate=False);\n","activity_df0.show(truncate=False);\n","cities_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVVHJ4YyxVzv","executionInfo":{"status":"ok","timestamp":1766403757307,"user_tz":-330,"elapsed":5176,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"36612c36-677e-4a79-975d-cd668965e8c3"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+-----+---------+-------------+\n","|driver_id|name  |age  |city     |vehicle_types|\n","+---------+------+-----+---------+-------------+\n","|D001     |Ramesh|35   |Hyderabad|Car,Bike     |\n","|D002     |Suresh|Forty|Bangalore|Auto         |\n","|D003     |Anita |NULL |Mumbai   |[Car]        |\n","|D004     |Kiran |29   |Delhi    |Car|Bike     |\n","|D005     |      |42   |Chennai  |NULL         |\n","+---------+------+-----+---------+-------------+\n","\n","+-------+---------+---------+-------------+-----------+--------+\n","|trip_id|driver_id|city     |trip_date_raw|trip_status|fare_raw|\n","+-------+---------+---------+-------------+-----------+--------+\n","|T001   |D001     |Hyderabad|2024-01-05   |Completed  |450     |\n","|T002   |D002     |Bangalore|05/01/2024   |Cancelled  |0       |\n","|T003   |D003     |Mumbai   |2024/01/06   |Completed  |620     |\n","|T004   |D004     |Delhi    |invalid_date |Completed  |540     |\n","|T005   |D001     |Hyderabad|2024-01-10   |Completed  |700     |\n","|T006   |D005     |Chennai  |2024-01-12   |Completed  |350     |\n","+-------+---------+---------+-------------+-----------+--------+\n","\n","+---------+------------------------+-------------------+--------------------+\n","|driver_id|actions                 |metadata_raw       |session_duration_sec|\n","+---------+------------------------+-------------------+--------------------+\n","|D001     |login,accept_trip,logout|{'device':'mobile'}|180                 |\n","|D002     |[login, logout]         |device=laptop      |60                  |\n","|D003     |login|accept_trip       |NULL               |120                 |\n","|D004     |NULL                    |{'device':'tablet'}|90                  |\n","|D005     |login                   |{'device':'mobile'}|30                  |\n","+---------+------------------------+-------------------+--------------------+\n","\n","+---------+------+\n","|city     |region|\n","+---------+------+\n","|Hyderabad|South |\n","|Bangalore|South |\n","|Mumbai   |West  |\n","|Delhi    |North |\n","|Chennai  |South |\n","+---------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","drivers_df0.show()\n","cities_df.show()\n","trips_df0.show()\n","activity_df0.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cQIvfy80baf","executionInfo":{"status":"ok","timestamp":1766403822632,"user_tz":-330,"elapsed":2036,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"c6dd7d12-1c2f-47e9-9717-f87cf1441992"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+-----+---------+-------------+\n","|driver_id|  name|  age|     city|vehicle_types|\n","+---------+------+-----+---------+-------------+\n","|     D001|Ramesh|   35|Hyderabad|     Car,Bike|\n","|     D002|Suresh|Forty|Bangalore|         Auto|\n","|     D003| Anita| NULL|   Mumbai|        [Car]|\n","|     D004| Kiran|   29|    Delhi|     Car|Bike|\n","|     D005|      |   42|  Chennai|         NULL|\n","+---------+------+-----+---------+-------------+\n","\n","+---------+------+\n","|     city|region|\n","+---------+------+\n","|Hyderabad| South|\n","|Bangalore| South|\n","|   Mumbai|  West|\n","|    Delhi| North|\n","|  Chennai| South|\n","+---------+------+\n","\n","+-------+---------+---------+-------------+-----------+--------+\n","|trip_id|driver_id|     city|trip_date_raw|trip_status|fare_raw|\n","+-------+---------+---------+-------------+-----------+--------+\n","|   T001|     D001|Hyderabad|   2024-01-05|  Completed|     450|\n","|   T002|     D002|Bangalore|   05/01/2024|  Cancelled|       0|\n","|   T003|     D003|   Mumbai|   2024/01/06|  Completed|     620|\n","|   T004|     D004|    Delhi| invalid_date|  Completed|     540|\n","|   T005|     D001|Hyderabad|   2024-01-10|  Completed|     700|\n","|   T006|     D005|  Chennai|   2024-01-12|  Completed|     350|\n","+-------+---------+---------+-------------+-----------+--------+\n","\n","+---------+--------------------+-------------------+--------------------+\n","|driver_id|             actions|       metadata_raw|session_duration_sec|\n","+---------+--------------------+-------------------+--------------------+\n","|     D001|login,accept_trip...|{'device':'mobile'}|                 180|\n","|     D002|     [login, logout]|      device=laptop|                  60|\n","|     D003|   login|accept_trip|               NULL|                 120|\n","|     D004|                NULL|{'device':'tablet'}|                  90|\n","|     D005|               login|{'device':'mobile'}|                  30|\n","+---------+--------------------+-------------------+--------------------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col,when\n","drivers_df=drivers_df0.withColumn(\"age\",when((col(\"age\")>0)&(col(\"age\")<=100),col(\"age\")).otherwise(None))\n"],"metadata":{"id":"ioNjoXXM00AM","executionInfo":{"status":"ok","timestamp":1766403883761,"user_tz":-330,"elapsed":49,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trips_df=trips_df0.withColumn(\n","    \"fare\",\n","    when(col(\"fare\")>=0,col(\"fare\")).otherwise(None)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"3Wes_VoU4E-V","executionInfo":{"status":"error","timestamp":1766403928007,"user_tz":-330,"elapsed":293,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"cb45b5fb-30be-45b8-d16c-1424bf7b6ad2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["{\"ts\": \"2025-12-22 11:45:30.317\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `fare` cannot be resolved. Did you mean one of the following? [`city`, `fare_raw`, `trip_id`, `driver_id`, `trip_status`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 3 in cell [23]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o336.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `fare` cannot be resolved. Did you mean one of the following? [`city`, `fare_raw`, `trip_id`, `driver_id`, `trip_status`]. SQLSTATE: 42703;\\n'Project [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73, CASE WHEN '`>=`('fare, 0) THEN 'fare ELSE null END AS fare#207]\\n+- LogicalRDD [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"]},{"output_type":"error","ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `fare` cannot be resolved. Did you mean one of the following? [`city`, `fare_raw`, `trip_id`, `driver_id`, `trip_status`]. SQLSTATE: 42703;\n'Project [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73, CASE WHEN '`>=`('fare, 0) THEN 'fare ELSE null END AS fare#207]\n+- LogicalRDD [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73], false\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3179826314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trips_df=trips_df0.withColumn(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"fare\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             )\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `fare` cannot be resolved. Did you mean one of the following? [`city`, `fare_raw`, `trip_id`, `driver_id`, `trip_status`]. SQLSTATE: 42703;\n'Project [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73, CASE WHEN '`>=`('fare, 0) THEN 'fare ELSE null END AS fare#207]\n+- LogicalRDD [trip_id#68, driver_id#69, city#70, trip_date_raw#71, trip_status#72, fare_raw#73], false\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n","trips_df=trips_df0.withColumn(\"trip_date\",to_date(col(\"trip_date_raw\"),\"yyyy-MM-dd\"))\n","trips_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"Maltabx84TEb","executionInfo":{"status":"error","timestamp":1766404014885,"user_tz":-330,"elapsed":704,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"23f78604-b3c5-41cf-da8e-eeb227bd61b0"},"execution_count":26,"outputs":[{"output_type":"error","ename":"DateTimeException","evalue":"[CANNOT_PARSE_TIMESTAMP] Text '05/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDateTimeException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-656734847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrips_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrips_df0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trip_date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trip_date_raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrips_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDateTimeException\u001b[0m: [CANNOT_PARSE_TIMESTAMP] Text '05/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import split\n","drivers_df=drivers_df0.withColumn(\"vehicle_types\",split(col(\"vehicle_types\"),\",\"))\n","activity_df=activity_df0.withColumn(\"actions\",split(col(\"actions\"),\",\"))\n","drivers_df.show()\n","activity_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTIb1QWC4qUl","executionInfo":{"status":"ok","timestamp":1766404227186,"user_tz":-330,"elapsed":1614,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"eadd6337-ff3f-4cd4-9b6b-efd23a150c8b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+-----+---------+-------------+\n","|driver_id|  name|  age|     city|vehicle_types|\n","+---------+------+-----+---------+-------------+\n","|     D001|Ramesh|   35|Hyderabad|  [Car, Bike]|\n","|     D002|Suresh|Forty|Bangalore|       [Auto]|\n","|     D003| Anita| NULL|   Mumbai|      [[Car]]|\n","|     D004| Kiran|   29|    Delhi|   [Car|Bike]|\n","|     D005|      |   42|  Chennai|         NULL|\n","+---------+------+-----+---------+-------------+\n","\n","+---------+--------------------+-------------------+--------------------+\n","|driver_id|             actions|       metadata_raw|session_duration_sec|\n","+---------+--------------------+-------------------+--------------------+\n","|     D001|[login, accept_tr...|{'device':'mobile'}|                 180|\n","|     D002|  [[login,  logout]]|      device=laptop|                  60|\n","|     D003| [login|accept_trip]|               NULL|                 120|\n","|     D004|                NULL|{'device':'tablet'}|                  90|\n","|     D005|             [login]|{'device':'mobile'}|                  30|\n","+---------+--------------------+-------------------+--------------------+\n","\n"]}]},{"cell_type":"code","source":["drivers_df=drivers_df0.dropna(subset=[\"driver_id\"])\n","drivers_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYa3pvPQ5Y9S","executionInfo":{"status":"ok","timestamp":1766404284383,"user_tz":-330,"elapsed":760,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"534d93ba-73bf-4494-d4c6-0662e0be3365"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+-----+---------+-------------+\n","|driver_id|  name|  age|     city|vehicle_types|\n","+---------+------+-----+---------+-------------+\n","|     D001|Ramesh|   35|Hyderabad|     Car,Bike|\n","|     D002|Suresh|Forty|Bangalore|         Auto|\n","|     D003| Anita| NULL|   Mumbai|        [Car]|\n","|     D004| Kiran|   29|    Delhi|     Car|Bike|\n","|     D005|      |   42|  Chennai|         NULL|\n","+---------+------+-----+---------+-------------+\n","\n"]}]},{"cell_type":"code","source":["drivers_df.printSchema()\n","drivers_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4xEZ0HY5ymW","executionInfo":{"status":"ok","timestamp":1766404376524,"user_tz":-330,"elapsed":1108,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"53d03438-af89-415e-aa98-38c6fd0aaba3"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- driver_id: string (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- age: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- vehicle_types: string (nullable = true)\n","\n","+---------+------+-----+---------+-------------+\n","|driver_id|name  |age  |city     |vehicle_types|\n","+---------+------+-----+---------+-------------+\n","|D001     |Ramesh|35   |Hyderabad|Car,Bike     |\n","|D002     |Suresh|Forty|Bangalore|Auto         |\n","|D003     |Anita |NULL |Mumbai   |[Car]        |\n","|D004     |Kiran |29   |Delhi    |Car|Bike     |\n","|D005     |      |42   |Chennai  |NULL         |\n","+---------+------+-----+---------+-------------+\n","\n"]}]},{"cell_type":"markdown","source":["Part-B"],"metadata":{"id":"v3sCISFT5-By"}},{"cell_type":"code","source":["trips_drivers_df=trips_df0.join(\n","    drivers_df,\n","    on=\"driver_id\",\n","    how=\"inner\"\n",")\n","trips_drivers_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pay1BTqj6Bbz","executionInfo":{"status":"ok","timestamp":1766404544035,"user_tz":-330,"elapsed":3003,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"5b02639f-f1be-4b87-8c1b-7c9eede97ef0"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-------+---------+-------------+-----------+--------+------+-----+---------+-------------+\n","|driver_id|trip_id|     city|trip_date_raw|trip_status|fare_raw|  name|  age|     city|vehicle_types|\n","+---------+-------+---------+-------------+-----------+--------+------+-----+---------+-------------+\n","|     D001|   T001|Hyderabad|   2024-01-05|  Completed|     450|Ramesh|   35|Hyderabad|     Car,Bike|\n","|     D001|   T005|Hyderabad|   2024-01-10|  Completed|     700|Ramesh|   35|Hyderabad|     Car,Bike|\n","|     D002|   T002|Bangalore|   05/01/2024|  Cancelled|       0|Suresh|Forty|Bangalore|         Auto|\n","|     D003|   T003|   Mumbai|   2024/01/06|  Completed|     620| Anita| NULL|   Mumbai|        [Car]|\n","|     D004|   T004|    Delhi| invalid_date|  Completed|     540| Kiran|   29|    Delhi|     Car|Bike|\n","|     D005|   T006|  Chennai|   2024-01-12|  Completed|     350|      |   42|  Chennai|         NULL|\n","+---------+-------+---------+-------------+-----------+--------+------+-----+---------+-------------+\n","\n"]}]},{"cell_type":"code","source":["trips_drivers_cities_df=trips_drivers_df.join(\n","    cities_df,\n","    on=\"city\",\n","    how=\"inner\"\n",")\n","trips_drivers_cities_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qA6lfase6mNW","executionInfo":{"status":"ok","timestamp":1766404572282,"user_tz":-330,"elapsed":2482,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"ff76e9b4-5690-49f5-dfaf-0f24111ff462"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|     city|driver_id|trip_id|trip_date_raw|trip_status|fare_raw|  name|  age|     city|vehicle_types|region|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|Bangalore|     D002|   T002|   05/01/2024|  Cancelled|       0|Suresh|Forty|Bangalore|         Auto| South|\n","|Hyderabad|     D001|   T005|   2024-01-10|  Completed|     700|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|Hyderabad|     D001|   T001|   2024-01-05|  Completed|     450|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|  Chennai|     D005|   T006|   2024-01-12|  Completed|     350|      |   42|  Chennai|         NULL| South|\n","|   Mumbai|     D003|   T003|   2024/01/06|  Completed|     620| Anita| NULL|   Mumbai|        [Car]|  West|\n","|    Delhi|     D004|   T004| invalid_date|  Completed|     540| Kiran|   29|    Delhi|     Car|Bike| North|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n","joined_final = trips_drivers_df.join(broadcast(cities_df), on=\"city\", how=\"left\")\n","joined_final.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vC8zZ5Ha6tVs","executionInfo":{"status":"ok","timestamp":1766404693726,"user_tz":-330,"elapsed":2361,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"6a1b9069-4f62-49d1-cfa9-3b43b7372781"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|     city|driver_id|trip_id|trip_date_raw|trip_status|fare_raw|  name|  age|     city|vehicle_types|region|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|Hyderabad|     D001|   T001|   2024-01-05|  Completed|     450|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|Hyderabad|     D001|   T005|   2024-01-10|  Completed|     700|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|Bangalore|     D002|   T002|   05/01/2024|  Cancelled|       0|Suresh|Forty|Bangalore|         Auto| South|\n","|   Mumbai|     D003|   T003|   2024/01/06|  Completed|     620| Anita| NULL|   Mumbai|        [Car]|  West|\n","|    Delhi|     D004|   T004| invalid_date|  Completed|     540| Kiran|   29|    Delhi|     Car|Bike| North|\n","|  Chennai|     D005|   T006|   2024-01-12|  Completed|     350|      |   42|  Chennai|         NULL| South|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n","broadcast_df = trips_drivers_df.join(broadcast(cities_df), on=\"city\", how=\"inner\")\n","broadcast_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-CDuHa57Kiy","executionInfo":{"status":"ok","timestamp":1766404791371,"user_tz":-330,"elapsed":2991,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"230a50ae-16b8-4b44-8044-43ddb805b6d2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|     city|driver_id|trip_id|trip_date_raw|trip_status|fare_raw|  name|  age|     city|vehicle_types|region|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","|Hyderabad|     D001|   T001|   2024-01-05|  Completed|     450|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|Hyderabad|     D001|   T005|   2024-01-10|  Completed|     700|Ramesh|   35|Hyderabad|     Car,Bike| South|\n","|Bangalore|     D002|   T002|   05/01/2024|  Cancelled|       0|Suresh|Forty|Bangalore|         Auto| South|\n","|   Mumbai|     D003|   T003|   2024/01/06|  Completed|     620| Anita| NULL|   Mumbai|        [Car]|  West|\n","|    Delhi|     D004|   T004| invalid_date|  Completed|     540| Kiran|   29|    Delhi|     Car|Bike| North|\n","|  Chennai|     D005|   T006|   2024-01-12|  Completed|     350|      |   42|  Chennai|         NULL| South|\n","+---------+---------+-------+-------------+-----------+--------+------+-----+---------+-------------+------+\n","\n"]}]},{"cell_type":"code","source":["valid_trips_df=trips_df.join(\n","    drivers_df.select(\"driver_id\"),\n","    on=\"driver_id\",\n","    how=\"inner\"\n",")\n","valid_trips_df.show()"],"metadata":{"id":"WOgtkGIx7ihj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PART-3"],"metadata":{"id":"vOu5eUMu8FVB"}},{"cell_type":"code","source":["from pyspark.sql.functions import explode,count\n","total_trips_per_city=trips_drivers_cities_df.groupBy(\"city\")\\\n",".agg(count(\"trip_id\").alias(\"total_trips\"))\n","total_trips_per_city.show()\n"],"metadata":{"id":"wGJGlob08BNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n","total_revenue_per_city=trips_drivers_cities_df.groupBy(\"city\")\\\n",".agg(sum(\"fare\").alias(\"total_revenue\"))\n","total_revenue_per_city.show()"],"metadata":{"id":"t2k3Ssjr87i4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import avg\n","avg_fare_per_driver=trips_df.groupBy(\"driver_id\")\\\n",".agg(avg(\"fare\").alias(\"avg_fare\"))\n","avg_fare_per_driver.show()"],"metadata":{"id":"OE8jYC4A9SzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["completed_trips_per_driver=trips_df.groupBy(\"driver_id\")\\\n",".agg(count)"],"metadata":{"id":"uGCVIP6w9mxJ"},"execution_count":null,"outputs":[]}]}