{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOghiiyMZvXhtP3G9YP+T9p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yKmbkqZxbsEo","executionInfo":{"status":"ok","timestamp":1766548757774,"user_tz":-330,"elapsed":17216,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark=SparkSession.builder\\\n","    .appName(\"Day22\")\\\n","    .getOrCreate()"]},{"cell_type":"code","source":["orders_data = [\n","    (\"ORD001\",\"C001\",\"Delhi \",\"Electronics\",\"Laptop\",\"45000\",\"2024-01-05\",\"Completed\"),\n","    (\"ORD002\",\"C002\",\"Mumbai\",\"Electronics\",\"Mobile \",\"32000\",\"05/01/2024\",\"Completed\"),\n","    (\"ORD003\",\"C003\",\"Bangalore\",\"Electronics\",\"Tablet\",\"30000\",\"2024/01/06\",\"Completed\"),\n","    (\"ORD004\",\"C004\",\"Delhi\",\"Electronics\",\"Laptop\",\"\",\"2024-01-07\",\"Cancelled\"),\n","    (\"ORD005\",\"C005\",\"Chennai\",\"Electronics\",\"Mobile\",\"invalid\",\"2024-01-08\",\"Completed\"),\n","    (\"ORD006\",\"C006\",\"Mumbai\",\"Home\",\"Mixer\",None,\"2024-01-08\",\"Completed\"),\n","    (\"ORD007\",\"C001\",\"Delhi\",\"Electronics\",\"Laptop\",\"47000\",\"09-01-2024\",\"Completed\"),\n","    (\"ORD008\",\"C007\",\"Bangalore\",\"Home\",\"Vacuum\",\"28000\",\"2024-01-09\",\"Completed\"),\n","    (\"ORD009\",\"C002\",\"Mumbai\",\"Electronics\",\"Laptop\",\"55000\",\"2024-01-10\",\"Completed\"),\n","    (\"ORD010\",\"C008\",\"Delhi\",\"Home\",\"AirPurifier\",\"38000\",\"2024-01-10\",\"Completed\"),\n","    (\"ORD011\",\"C009\",\"Mumbai\",\"Home\",\"Vacuum\",\"29000\",\"2024-01-11\",\"Completed\"),\n","    (\"ORD012\",\"C010\",\"Bangalore\",\"Electronics\",\"Mobile\",\"33000\",\"2024-01-11\",\"Completed\"),\n","    (\"ORD013\",\"C003\",\"Bangalore\",\"Home\",\"Mixer\",\"21000\",\"2024-01-12\",\"Completed\"),\n","    (\"ORD014\",\"C004\",\"Delhi\",\"Electronics\",\"Tablet\",\"26000\",\"2024-01-12\",\"Completed\"),\n","    (\"ORD015\",\"C005\",\"Chennai\",\"Electronics\",\"Laptop\",\"62000\",\"2024-01-13\",\"Completed\"),\n","    (\"ORD016\",\"C006\",\"Mumbai\",\"Home\",\"AirPurifier\",\"40000\",\"2024-01-13\",\"Completed\"),\n","    (\"ORD017\",\"C007\",\"Bangalore\",\"Electronics\",\"Laptop\",\"51000\",\"2024-01-14\",\"Completed\"),\n","    (\"ORD018\",\"C008\",\"Delhi\",\"Home\",\"Vacuum\",\"31000\",\"2024-01-14\",\"Completed\"),\n","    (\"ORD019\",\"C009\",\"Mumbai\",\"Electronics\",\"Tablet\",\"29000\",\"2024-01-15\",\"Completed\"),\n","    (\"ORD020\",\"C010\",\"Bangalore\",\"Electronics\",\"Laptop\",\"54000\",\"2024-01-15\",\"Completed\"),\n","    (\"ORD020\",\"C010\",\"Bangalore\",\"Electronics\",\"Laptop\",\"54000\",\"2024-01-15\",\"Completed\")\n","]\n","columns=[\"order_id\",\"customer_id\",\"city\",\"category\",\"product\",\"amount\",\"order_date\",\"status\"]"],"metadata":{"id":"lzAIumd7gpJJ","executionInfo":{"status":"ok","timestamp":1766548895734,"user_tz":-330,"elapsed":46,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,StringType\n","orders_schema=StructType([\n","    StructField(\"order_id\",StringType(),True),\n","    StructField(\"customer_id\",StringType(),True),\n","    StructField(\"city\",StringType(),True),\n","    StructField(\"category\",StringType(),True),\n","    StructField(\"product\",StringType(),True),\n","    StructField(\"amount\",StringType(),True),\n","    StructField(\"order_date\",StringType(),True),\n","    StructField(\"status\",StringType(),True)\n","])\n"],"metadata":{"id":"VWMNlHWKhRb7","executionInfo":{"status":"ok","timestamp":1766549563557,"user_tz":-330,"elapsed":40,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["orders_df=spark.createDataFrame(orders_data,schema=orders_schema)\n","orders_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9s0uQswhoQr","executionInfo":{"status":"ok","timestamp":1766549576164,"user_tz":-330,"elapsed":8926,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"4cfe5a3f-0aed-4e5b-81ca-2a32cbf034a7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-----------+---------+-----------+-----------+-------+----------+---------+\n","|order_id|customer_id|     city|   category|    product| amount|order_date|   status|\n","+--------+-----------+---------+-----------+-----------+-------+----------+---------+\n","|  ORD001|       C001|   Delhi |Electronics|     Laptop|  45000|2024-01-05|Completed|\n","|  ORD002|       C002|   Mumbai|Electronics|    Mobile |  32000|05/01/2024|Completed|\n","|  ORD003|       C003|Bangalore|Electronics|     Tablet|  30000|2024/01/06|Completed|\n","|  ORD004|       C004|    Delhi|Electronics|     Laptop|       |2024-01-07|Cancelled|\n","|  ORD005|       C005|  Chennai|Electronics|     Mobile|invalid|2024-01-08|Completed|\n","|  ORD006|       C006|   Mumbai|       Home|      Mixer|   NULL|2024-01-08|Completed|\n","|  ORD007|       C001|    Delhi|Electronics|     Laptop|  47000|09-01-2024|Completed|\n","|  ORD008|       C007|Bangalore|       Home|     Vacuum|  28000|2024-01-09|Completed|\n","|  ORD009|       C002|   Mumbai|Electronics|     Laptop|  55000|2024-01-10|Completed|\n","|  ORD010|       C008|    Delhi|       Home|AirPurifier|  38000|2024-01-10|Completed|\n","|  ORD011|       C009|   Mumbai|       Home|     Vacuum|  29000|2024-01-11|Completed|\n","|  ORD012|       C010|Bangalore|Electronics|     Mobile|  33000|2024-01-11|Completed|\n","|  ORD013|       C003|Bangalore|       Home|      Mixer|  21000|2024-01-12|Completed|\n","|  ORD014|       C004|    Delhi|Electronics|     Tablet|  26000|2024-01-12|Completed|\n","|  ORD015|       C005|  Chennai|Electronics|     Laptop|  62000|2024-01-13|Completed|\n","|  ORD016|       C006|   Mumbai|       Home|AirPurifier|  40000|2024-01-13|Completed|\n","|  ORD017|       C007|Bangalore|Electronics|     Laptop|  51000|2024-01-14|Completed|\n","|  ORD018|       C008|    Delhi|       Home|     Vacuum|  31000|2024-01-14|Completed|\n","|  ORD019|       C009|   Mumbai|Electronics|     Tablet|  29000|2024-01-15|Completed|\n","|  ORD020|       C010|Bangalore|Electronics|     Laptop|  54000|2024-01-15|Completed|\n","+--------+-----------+---------+-----------+-----------+-------+----------+---------+\n","only showing top 20 rows\n"]}]},{"cell_type":"code","source":["orders_df.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAWlH6Qfl77w","executionInfo":{"status":"ok","timestamp":1766550173525,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"19855ced-34bf-4a60-dae7-fe89a0c0077c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- order_id: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- product: string (nullable = true)\n"," |-- amount: string (nullable = true)\n"," |-- order_date: string (nullable = true)\n"," |-- status: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["#Phase 2-DATA CLEANING\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","def trim_all_string_cols(df):\n","    exprs=[]\n","    for col in df.schema.names:\n","        if isinstance(df.schema[col].dataType, StringType):\n","            df = df.withColumn(col, F.trim(df[col]))\n","    return df.select(*exprs)\n","df=trim_all_string_cols(orders_df)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yI4jFq-OmJCi","executionInfo":{"status":"ok","timestamp":1766550457074,"user_tz":-330,"elapsed":996,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"4cc66bcd-9f7d-4748-d8b7-7bddc95bc7cd"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["++\n","||\n","++\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","||\n","++\n","only showing top 20 rows\n"]}]},{"cell_type":"code","source":["df=(\n","    df.withColumn(\"city\",F.lower(F.col(\"city\")))\\\n","    .withColumn(\"category\",F.lower(F.col(\"category\")))\\\n","    .withColumn(\"product\",F.lower(F.col(\"product\")))\n",")\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":966},"id":"CKrTYByenQdI","executionInfo":{"status":"error","timestamp":1766550795233,"user_tz":-330,"elapsed":249,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"fe577cbf-e1eb-4651-e5c5-e88c3b53ee2c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["{\"ts\": \"2025-12-24 04:33:16.618\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `city` cannot be resolved.  SQLSTATE: 42703\", \"context\": {\"file\": \"line 2 in cell [19]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITHOUT_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o146.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `city` cannot be resolved.  SQLSTATE: 42703;\\n'Project ['lower('city) AS city#44]\\n+- Project\\n   +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, order_date#39, trim(status#7, None) AS status#40]\\n      +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, trim(order_date#6, None) AS order_date#39, status#7]\\n         +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, trim(amount#5, None) AS amount#38, order_date#6, status#7]\\n            +- Project [order_id#33, customer_id#34, city#35, category#36, trim(product#4, None) AS product#37, amount#5, order_date#6, status#7]\\n               +- Project [order_id#33, customer_id#34, city#35, trim(category#3, None) AS category#36, product#4, amount#5, order_date#6, status#7]\\n                  +- Project [order_id#33, customer_id#34, trim(city#2, None) AS city#35, category#3, product#4, amount#5, order_date#6, status#7]\\n                     +- Project [order_id#33, trim(customer_id#1, None) AS customer_id#34, city#2, category#3, product#4, amount#5, order_date#6, status#7]\\n                        +- Project [trim(order_id#0, None) AS order_id#33, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7]\\n                           +- LogicalRDD [order_id#0, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 24 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"]},{"output_type":"error","ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `city` cannot be resolved.  SQLSTATE: 42703;\n'Project ['lower('city) AS city#44]\n+- Project\n   +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, order_date#39, trim(status#7, None) AS status#40]\n      +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, trim(order_date#6, None) AS order_date#39, status#7]\n         +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, trim(amount#5, None) AS amount#38, order_date#6, status#7]\n            +- Project [order_id#33, customer_id#34, city#35, category#36, trim(product#4, None) AS product#37, amount#5, order_date#6, status#7]\n               +- Project [order_id#33, customer_id#34, city#35, trim(category#3, None) AS category#36, product#4, amount#5, order_date#6, status#7]\n                  +- Project [order_id#33, customer_id#34, trim(city#2, None) AS city#35, category#3, product#4, amount#5, order_date#6, status#7]\n                     +- Project [order_id#33, trim(customer_id#1, None) AS customer_id#34, city#2, category#3, product#4, amount#5, order_date#6, status#7]\n                        +- Project [trim(order_id#0, None) AS order_id#33, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7]\n                           +- LogicalRDD [order_id#0, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7], false\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2557767437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df=(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             )\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `city` cannot be resolved.  SQLSTATE: 42703;\n'Project ['lower('city) AS city#44]\n+- Project\n   +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, order_date#39, trim(status#7, None) AS status#40]\n      +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, amount#38, trim(order_date#6, None) AS order_date#39, status#7]\n         +- Project [order_id#33, customer_id#34, city#35, category#36, product#37, trim(amount#5, None) AS amount#38, order_date#6, status#7]\n            +- Project [order_id#33, customer_id#34, city#35, category#36, trim(product#4, None) AS product#37, amount#5, order_date#6, status#7]\n               +- Project [order_id#33, customer_id#34, city#35, trim(category#3, None) AS category#36, product#4, amount#5, order_date#6, status#7]\n                  +- Project [order_id#33, customer_id#34, trim(city#2, None) AS city#35, category#3, product#4, amount#5, order_date#6, status#7]\n                     +- Project [order_id#33, trim(customer_id#1, None) AS customer_id#34, city#2, category#3, product#4, amount#5, order_date#6, status#7]\n                        +- Project [trim(order_id#0, None) AS order_id#33, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7]\n                           +- LogicalRDD [order_id#0, customer_id#1, city#2, category#3, product#4, amount#5, order_date#6, status#7], false\n"]}]},{"cell_type":"code","source":["df=df.groupBy(\"city\").agg(sum(\"order_amount\").alias(\"total_amount\")).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"CKLDI3GeQ0A9","executionInfo":{"status":"error","timestamp":1766577695771,"user_tz":-330,"elapsed":50,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"fbc05770-1591-42d4-ab73-6d5571034a7f"},"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3398686060.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["df.groupBy(\"category\").agg(sum(\"order_amount\").alias(\"total_amount\"))"],"metadata":{"id":"hlirSslzPEEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupBy(\"product\").agg(sum(\"amount\").agg(sum(\"total_amount\"))).show()"],"metadata":{"id":"2qUvdP88PR5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupBy(\"city\").agg(avg(\"amount\").alias(\"avg_amount\")).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"9Tw-5RG-Pbzz","executionInfo":{"status":"error","timestamp":1766577808384,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"4be85560-c200-4c66-bac0-9ff75f0678ec"},"execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2016568952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["df.show(3)"],"metadata":{"id":"tQ6lulQHPjRX","executionInfo":{"status":"error","timestamp":1766577821831,"user_tz":-330,"elapsed":42,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"da30c036-4812-4373-d147-26f26b7c9c39","colab":{"base_uri":"https://localhost:8080/","height":141}},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1537271291.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]}]}