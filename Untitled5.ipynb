{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvk+Ybad5eSB/od5Q/1zZN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"gj05qozKMDWa","executionInfo":{"status":"ok","timestamp":1766728032119,"user_tz":-330,"elapsed":132000,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"44c7854d-e273-4190-9879-f86cf51dce30"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-8cff9b6e-2fb8-4aba-8592-bd7b2acf43a6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-8cff9b6e-2fb8-4aba-8592-bd7b2acf43a6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving orders_large_bad.csv to orders_large_bad.csv\n","Saving orders_large_bad.json to orders_large_bad.json\n","Uploaded files: ['orders_large_bad.csv', 'orders_large_bad.json']\n"]}],"source":["from google.colab import files\n","\n","# This will open a file picker. Select both files:\n","# orders_large_bad.csv and orders_large_bad.json\n","uploaded = files.upload()\n","\n","# Check uploaded files\n","print(\"Uploaded files:\", list(uploaded.keys()))"]},{"cell_type":"code","source":["!ls -lh /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOa0HWnKMgss","executionInfo":{"status":"ok","timestamp":1766728052671,"user_tz":-330,"elapsed":109,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"f7b761ef-e7f8-4c7b-b065-279f4d48405a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["total 70M\n","-rw-r--r-- 1 root root  20M Dec 26 05:47 orders_large_bad.csv\n","-rw-r--r-- 1 root root  50M Dec 26 05:47 orders_large_bad.json\n","drwxr-xr-x 1 root root 4.0K Dec 11 14:34 sample_data\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession, functions as F, types as T\n","\n","def main(\n","    csv_path: str = \"orders_large_bad.csv\",\n","    json_path: str = \"orders_large_bad.json\",\n","):\n","    spark = (\n","        SparkSession.builder.appName(\"Orders-Phase-1-Ingestion\")\n","        # Optional: tweak for local runs; adjust as needed\n","        .config(\"spark.sql.shuffle.partitions\", \"200\")\n","        .getOrCreate()\n","    )\n","\n","    # --------------------------------------------------------------------\n","    # 1) READ CSV (disable schema inference; everything as STRING)\n","    # --------------------------------------------------------------------\n","    # NOTE: CSV reader defaults to strings when inferSchema=False (default),\n","    # but we set it explicitly for clarity.\n","    df_csv = (\n","        spark.read\n","        .option(\"header\", True)\n","        .option(\"inferSchema\", False)\n","        .option(\"mode\", \"PERMISSIVE\")   # keep bad records instead of failing\n","        .option(\"multiLine\", False)     # CSV is single-line records\n","        .csv(csv_path)\n","    )\n","\n","    print(\"\\n=== CSV — PRINT SCHEMA ===\")\n","    df_csv.printSchema()\n","\n","    csv_count = df_csv.count()\n","    print(f\"\\n=== CSV — RECORD COUNT: {csv_count:,} ===\\n\")\n","\n","    print(\"=== CSV — 20 RANDOM ROWS (for eyeballing) ===\")\n","    (\n","        df_csv\n","        .orderBy(F.rand())\n","        .limit(20)\n","        .show(truncate=False)\n","    )\n","\n","    # --------------------------------------------------------------------\n","    # 2) READ JSON (force primitives as STRING to avoid inference)\n","    # --------------------------------------------------------------------\n","    # Many JSON readers infer numbers/dates automatically.\n","    # \"primitivesAsString\" forces numbers/booleans to string for Phase 1.\n","    df_json = (\n","        spark.read\n","        .option(\"multiLine\", False)                 # line-delimited JSON\n","        .option(\"primitivesAsString\", True)         # <- critical for Phase 1\n","        .option(\"mode\", \"PERMISSIVE\")\n","        .json(json_path)\n","    )\n","\n","    print(\"\\n=== JSON — PRINT SCHEMA ===\")\n","    df_json.printSchema()\n","\n","    json_count = df_json.count()\n","    print(f\"\\n=== JSON — RECORD COUNT: {json_count:,} ===\\n\")\n","\n","    print(\"=== JSON — 20 RANDOM ROWS (for eyeballing) ===\")\n","    (\n","        df_json\n","        .orderBy(F.rand())\n","        .limit(20)\n","        .show(truncate=False)\n","    )\n","\n","    # --------------------------------------------------------------------\n","    # 3) COMPARE CSV vs JSON basic metadata\n","    # --------------------------------------------------------------------\n","    csv_cols = set(df_csv.columns)\n","    json_cols = set(df_json.columns)\n","    only_in_csv = sorted(list(csv_cols - json_cols))\n","    only_in_json = sorted(list(json_cols - csv_cols))\n","\n","    print(\"\\n=== SCHEMA COMPARISON (CSV vs JSON) ===\")\n","    print(f\"Columns only in CSV : {only_in_csv}\")\n","    print(f\"Columns only in JSON: {only_in_json}\")\n","    print(f\"Have same columns?  {csv_cols == json_cols}\")\n","    print(f\"CSV rows: {csv_count:,} | JSON rows: {json_count:,}\\n\")\n","\n","    # --------------------------------------------------------------------\n","    # 4) QUICK DATA-QUALITY PROBES (identify >= 5 issues)\n","    #    We’re still in Phase 1 (observation), so we don’t fix anything yet.\n","    # --------------------------------------------------------------------\n","    # Helper expressions\n","    trim_all = {c: F.trim(F.col(c)).alias(c) for c in df_csv.columns}\n","    df_trim = df_csv.select(*trim_all.values())\n","\n","    # 4.1 Missing/empty values in some critical columns\n","    critical_cols = [\"order_id\", \"customer_id\", \"city\", \"category\", \"product\", \"amount\", \"order_date\", \"status\"]\n","    missing_stats = []\n","    for c in critical_cols:\n","        missing_stats.append(\n","            df_trim.where(F.col(c).isNull() | (F.length(F.col(c)) == 0)).agg(\n","                F.count(F.lit(1)).alias(\"missing_rows\")\n","            ).withColumn(\"column\", F.lit(c))\n","        )\n","    missing_summary = missing_stats[0]\n","    for m in missing_stats[1:]:\n","        missing_summary = missing_summary.unionByName(m)\n","    print(\"=== DQ Probe 1 — Missing/Empty in Critical Columns (CSV) ===\")\n","    missing_summary.select(\"column\", \"missing_rows\").orderBy(F.desc(\"missing_rows\")).show(100, truncate=False)\n","\n","    # 4.2 'amount' obvious issues:\n","    #  - literal 'invalid'\n","    #  - contains commas (e.g., \"12,000\")\n","    #  - non-numeric characters after removing commas\n","    amount_trim = F.trim(F.col(\"amount\"))\n","    amount_no_commas = F.regexp_replace(amount_trim, \",\", \"\")\n","    amt_invalid_literal = (F.lower(amount_trim) == F.lit(\"invalid\"))\n","    amt_has_comma = F.col(\"amount\").rlike(\",\")\n","    amt_non_numeric = ~amount_no_commas.rlike(\"^[0-9]+$\")\n","\n","    print(\"=== DQ Probe 2 — 'amount' validity buckets (CSV) ===\")\n","    (\n","        df_trim\n","        .withColumn(\"amount_invalid_literal\", amt_invalid_literal.cast(\"int\"))\n","        .withColumn(\"amount_has_comma\", amt_has_comma.cast(\"int\"))\n","        .withColumn(\"amount_non_numeric_after_strip\", (amt_non_numeric & ~amt_invalid_literal).cast(\"int\"))\n","        .groupBy()\n","        .agg(\n","            F.sum(\"amount_invalid_literal\").alias(\"count_invalid_literal\"),\n","            F.sum(\"amount_has_comma\").alias(\"count_contains_comma\"),\n","            F.sum(\"amount_non_numeric_after_strip\").alias(\"count_non_numeric_after_strip\"),\n","        )\n","        .show(truncate=False)\n","    )\n","\n","    # 4.3 'order_date' mixed formats / invalid tokens\n","    #  Patterns we commonly see in this dataset:\n","    #   - ISO:          YYYY-MM-DD\n","    #   - Slash (DMY):  DD/MM/YYYY\n","    #   - Slash (YMD):  YYYY/MM/DD\n","    #   - 'invalid_date'\n","    order_date = F.col(\"order_date\")\n","    is_iso = order_date.rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n","    is_slash_dmy = order_date.rlike(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n","    is_slash_ymd = order_date.rlike(r\"^\\d{4}/\\d{2}/\\d{2}$\")\n","    is_invalid_token = F.lower(order_date) == F.lit(\"invalid_date\")\n","    is_missing = order_date.isNull() | (F.length(F.trim(order_date)) == 0)\n","\n","    print(\"=== DQ Probe 3 — 'order_date' format buckets (CSV) ===\")\n","    (\n","        df_trim\n","        .withColumn(\"fmt_iso_yyyy_mm_dd\", is_iso.cast(\"int\"))\n","        .withColumn(\"fmt_slash_dd_mm_yyyy\", is_slash_dmy.cast(\"int\"))\n","        .withColumn(\"fmt_slash_yyyy_mm_dd\", is_slash_ymd.cast(\"int\"))\n","        .withColumn(\"invalid_date_token\", is_invalid_token.cast(\"int\"))\n","        .withColumn(\"missing_or_blank\", is_missing.cast(\"int\"))\n","        .groupBy()\n","        .agg(\n","            F.sum(\"fmt_iso_yyyy_mm_dd\").alias(\"count_iso\"),\n","            F.sum(\"fmt_slash_dd_mm_yyyy\").alias(\"count_slash_dmy\"),\n","            F.sum(\"fmt_slash_yyyy_mm_dd\").alias(\"count_slash_ymd\"),\n","            F.sum(\"invalid_date_token\").alias(\"count_invalid_token\"),\n","            F.sum(\"missing_or_blank\").alias(\"count_missing_or_blank\"),\n","        )\n","        .show(truncate=False)\n","    )\n","\n","    # 4.4 Extra leading/trailing spaces in common string columns\n","    str_cols = [\"city\", \"category\", \"product\"]\n","    exprs_space = []\n","    for c in str_cols:\n","        exprs_space.append(\n","            F.sum((F.col(c) != F.trim(F.col(c))).cast(\"int\")).alias(f\"rows_with_space_{c}\")\n","        )\n","    print(\"=== DQ Probe 4 — Leading/Trailing Spaces (CSV) ===\")\n","    df_csv.agg(*exprs_space).show(truncate=False)\n","\n","    # 4.5 Case inconsistency in 'city' (distinct before vs after lower+trim)\n","    city_distinct_before = df_csv.select(\"city\").distinct().count()\n","    city_distinct_after = df_trim.select(F.lower(F.col(\"city\")).alias(\"city_norm\")).distinct().count()\n","    print(\"=== DQ Probe 5 — City distinct values (case/space inconsistency) (CSV) ===\")\n","    print(f\"Distinct city values (raw):  {city_distinct_before}\")\n","    print(f\"Distinct city values (norm): {city_distinct_after}\")\n","\n","    print(\"Top examples of city variants mapping to the same normalized token:\")\n","    (\n","        df_trim\n","        .withColumn(\"city_norm\", F.lower(F.col(\"city\")))\n","        .groupBy(\"city_norm\")\n","        .agg(F.collect_set(\"city\").alias(\"variants\"), F.count(\"*\").alias(\"cnt\"))\n","        .orderBy(F.desc(\"cnt\"))\n","        .limit(15)\n","        .show(truncate=False)\n","    )\n","\n","    # 4.6 Detect stray spaces in product/category tokens (like \"Mobile \" or \" home \")\n","    print(\"=== DQ Probe 6 — Tokens with internal/edge spaces (CSV) ===\")\n","    (\n","        df_csv\n","        .select(\n","            F.expr(\"filter(split(category, ' +'), x -> x <> '')\").alias(\"category_tokens\"),\n","            F.expr(\"filter(split(product, ' +'), x -> x <> '')\").alias(\"product_tokens\"),\n","            \"category\", \"product\"\n","        )\n","        .where((F.col(\"category\") != F.trim(F.col(\"category\"))) | (F.col(\"product\") != F.trim(F.col(\"product\"))))\n","        .limit(20)\n","        .show(truncate=False)\n","    )\n","\n","    # 4.7 (Optional) Show a few suspicious rows for manual review:\n","    print(\"=== DQ Probe 7 — Sample suspicious rows (amount invalid/comma/non-numeric OR invalid_date) (CSV) ===\")\n","    suspicious = df_trim.where(\n","        (F.lower(F.col(\"amount\")) == \"invalid\")\n","        | (F.col(\"amount\").rlike(\",\"))\n","        | (~F.regexp_replace(F.col(\"amount\"), \",\", \"\").rlike(\"^[0-9]+$\"))\n","        | (F.lower(F.col(\"order_date\")) == \"invalid_date\")\n","        | (F.col(\"order_date\").isNull())\n","        | (F.length(F.trim(F.col(\"order_date\"))) == 0)\n","    )\n","    suspicious.orderBy(F.rand()).limit(20).show(truncate=False)\n","\n","    print(\"\\n=== Phase 1 COMPLETE ===\")\n","    print(\"Identified issues include (non-exhaustive):\")\n","    print(\" - Missing / blank values (amount, order_date, category, product, etc.)\")\n","    print(\" - 'amount' with 'invalid', commas ('12,000'), or non-numeric characters\")\n","    print(\" - Mixed date formats (YYYY-MM-DD, DD/MM/YYYY, YYYY/MM/DD) and 'invalid_date'\")\n","    print(\" - Leading/trailing whitespace & inconsistent casing in city/category/product\")\n","    print(\" - Occasional trailing spaces in tokens (e.g., 'Mobile ' or ' home ')\")\n","    print(\"\\nNext Phase: define an explicit schema and validate rows against it.\")\n","\n","    spark.stop()\n","\n","\n","if __name__ == \"__main__\":\n","    main(csv_path=\"/content/orders_large_bad.csv\",\n","         json_path=\"/content/orders_large_bad.json\")\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujPgU-KAMxvM","executionInfo":{"status":"ok","timestamp":1766728240128,"user_tz":-330,"elapsed":67810,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"a9fe0e9d-012c-4cd5-a796-cfbd188ed41c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== CSV — PRINT SCHEMA ===\n","root\n"," |-- order_id: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- product: string (nullable = true)\n"," |-- amount: string (nullable = true)\n"," |-- order_date: string (nullable = true)\n"," |-- status: string (nullable = true)\n","\n","\n","=== CSV — RECORD COUNT: 300,000 ===\n","\n","=== CSV — 20 RANDOM ROWS (for eyeballing) ===\n","+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n","|order_id   |customer_id|city     |category   |product    |amount |order_date|status   |\n","+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n","|ORD00045790|C045790    |Delhi    |Fashion    |Shoes      |invalid|2024-01-11|Completed|\n","|ORD00171632|C021632    | mumbai  |Home       |AirPurifier|65633  |2024-02-02|Completed|\n","|ORD00190278|C040278    |Hyderabad| grocery   |Sugar      |24858  |19/01/2024|Completed|\n","|ORD00145245|C045245    |Hyderabad|Grocery    |Rice       |12,000 |2024-02-15|Completed|\n","|ORD00124711|C024711    |Mumbai   |Grocery    |Sugar      |65688  |2024-02-01|Completed|\n","|ORD00138671|C038671    |Delhi    |Fashion    |TShirt     |28587  |2024/01/12|Completed|\n","|ORD00083966|C033966    |Kolkata  |Home       |AirPurifier|81120  |2024-01-27|Completed|\n","|ORD00183081|C033081    |Chennai  |Electronics|Laptop     |66669  |2024-01-22|Completed|\n","|ORD00029365|C029365    |Bangalore|Fashion    |TShirt     |64454  |2024-01-26|Completed|\n","|ORD00021251|C021251    |Delhi    |Electronics|Laptop     |29833  |2024-01-12|Completed|\n","|ORD00059590|C009590    |Kolkata  |Fashion    |Shoes      |79209  |2024-01-11|Completed|\n","|ORD00237878|C037878    |Bangalore|Fashion    |Shoes      |5767   |2024-02-08|Completed|\n","|ORD00026555|C026555    |Chennai  |Home       |AirPurifier|48227  |2024-02-05|Completed|\n","|ORD00219959|C019959    |Bangalore|Grocery    |Sugar      |75544  |2024-02-29|Completed|\n","|ORD00268972|C018972    |Chennai  |Electronics|Mobile     |49268  |22/02/2024|Completed|\n","|ORD00264819|C014819    |Bangalore|Fashion    |Shoes      |69741  |2024-02-09|Completed|\n","|ORD00292798|C042798    |Hyderabad|Home       |AirPurifier|88070  |28/02/2024|Completed|\n","|ORD00117999|C017999    |Bangalore|Electronics|Laptop     |76926  |2024-02-09|Completed|\n","|ORD00029722|C029722    |Kolkata  |Home       |Vacuum     |68519  |23/01/2024|Completed|\n","|ORD00051853|C001853    |Bangalore|Fashion    |Shoes      |69434  |2024-01-14|Completed|\n","+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n","\n","\n","=== JSON — PRINT SCHEMA ===\n","root\n"," |-- amount: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- order_date: string (nullable = true)\n"," |-- order_id: string (nullable = true)\n"," |-- product: string (nullable = true)\n"," |-- status: string (nullable = true)\n","\n","\n","=== JSON — RECORD COUNT: 300,000 ===\n","\n","=== JSON — 20 RANDOM ROWS (for eyeballing) ===\n","+-------+-----------+---------+-----------+----------+-----------+-----------+---------+\n","|amount |category   |city     |customer_id|order_date|order_id   |product    |status   |\n","+-------+-----------+---------+-----------+----------+-----------+-----------+---------+\n","|23854  |Electronics|Bangalore|C006447    |2024-01-28|ORD00006447|Laptop     |Completed|\n","|12,000 |Electronics|Hyderabad|C013971    |2024-02-01|ORD00263971|Laptop     |Completed|\n","|12344  |Home       |Delhi    |C039017    |18/01/2024|ORD00039017|AirPurifier|Completed|\n","|28288  |Home       |Bangalore|C024807    |2024/01/08|ORD00274807|Vacuum     |Completed|\n","|66444  |Home       |Kolkata  |C033367    |2024-01-28|ORD00233367|Vacuum     |Completed|\n","|89959  |Home       |Bangalore|C031043    |2024-02-13|ORD00081043|AirPurifier|Completed|\n","|53177  |Grocery    |Hyderabad|C004300    |2024-02-10|ORD00004300|Rice       |Cancelled|\n","|65105  |Home       |Pune     |C035401    |2024-01-02|ORD00035401|AirPurifier|Completed|\n","|5620   |Electronics|Hyderabad|C019978    |2024-02-08|ORD00119978|Tablet     |Completed|\n","|35571  |Home       |Mumbai   |C037216    |2024-02-06|ORD00237216|Vacuum     |Completed|\n","|20536  |Fashion    |Kolkata  |C004259    |2024-02-29|ORD00154259|Jeans      |Completed|\n","|invalid|Grocery    |Pune     |C023177    |2024-02-27|ORD00123177|Sugar      |Completed|\n","|18868  |Grocery    |Kolkata  |C048991    |2024/02/01|ORD00198991|Sugar      |Completed|\n","|invalid|Grocery    |Bangalore|C030869    |2024-02-19|ORD00230869|Sugar      |Completed|\n","|54835  |Home       |Chennai  |C028781    |2024-01-22|ORD00128781|AirPurifier|Completed|\n","|71273  |Fashion    |Mumbai   |C020456    |2024-02-26|ORD00020456|TShirt     |Completed|\n","|45279  |Fashion    |Chennai  |C014145    |2024-01-06|ORD00214145|Shoes      |Completed|\n","|67798  |Fashion    |Delhi    |C032397    |2024-02-07|ORD00132397|Jeans      |Completed|\n","|7755   |Fashion    |Mumbai   |C015632    |13/01/2024|ORD00115632|Jeans      |Completed|\n","|68944  |Fashion    |Chennai  |C000883    |2024-01-24|ORD00100883|Shoes      |Completed|\n","+-------+-----------+---------+-----------+----------+-----------+-----------+---------+\n","\n","\n","=== SCHEMA COMPARISON (CSV vs JSON) ===\n","Columns only in CSV : []\n","Columns only in JSON: []\n","Have same columns?  True\n","CSV rows: 300,000 | JSON rows: 300,000\n","\n","=== DQ Probe 1 — Missing/Empty in Critical Columns (CSV) ===\n","+-----------+------------+\n","|column     |missing_rows|\n","+-----------+------------+\n","|amount     |9374        |\n","|order_id   |0           |\n","|customer_id|0           |\n","|city       |0           |\n","|category   |0           |\n","|product    |0           |\n","|order_date |0           |\n","|status     |0           |\n","+-----------+------------+\n","\n","=== DQ Probe 2 — 'amount' validity buckets (CSV) ===\n","+---------------------+--------------------+-----------------------------+\n","|count_invalid_literal|count_contains_comma|count_non_numeric_after_strip|\n","+---------------------+--------------------+-----------------------------+\n","|15790                |12357               |0                            |\n","+---------------------+--------------------+-----------------------------+\n","\n","=== DQ Probe 3 — 'order_date' format buckets (CSV) ===\n","+---------+---------------+---------------+-------------------+----------------------+\n","|count_iso|count_slash_dmy|count_slash_ymd|count_invalid_token|count_missing_or_blank|\n","+---------+---------------+---------------+-------------------+----------------------+\n","|249153   |27273          |20979          |2595               |0                     |\n","+---------+---------------+---------------+-------------------+----------------------+\n","\n","=== DQ Probe 4 — Leading/Trailing Spaces (CSV) ===\n","+--------------------+------------------------+-----------------------+\n","|rows_with_space_city|rows_with_space_category|rows_with_space_product|\n","+--------------------+------------------------+-----------------------+\n","|17648               |9678                    |8109                   |\n","+--------------------+------------------------+-----------------------+\n","\n","=== DQ Probe 5 — City distinct values (case/space inconsistency) (CSV) ===\n","Distinct city values (raw):  14\n","Distinct city values (norm): 7\n","Top examples of city variants mapping to the same normalized token:\n","+---------+----------------------+-----+\n","|city_norm|variants              |cnt  |\n","+---------+----------------------+-----+\n","|hyderabad|[hyderabad, Hyderabad]|43239|\n","|pune     |[pune, Pune]          |43068|\n","|delhi    |[delhi, Delhi]        |42912|\n","|chennai  |[chennai, Chennai]    |42860|\n","|mumbai   |[mumbai, Mumbai]      |42816|\n","|kolkata  |[kolkata, Kolkata]    |42660|\n","|bangalore|[Bangalore, bangalore]|42445|\n","+---------+----------------------+-----+\n","\n","=== DQ Probe 6 — Tokens with internal/edge spaces (CSV) ===\n","+---------------+--------------+-------------+------------+\n","|category_tokens|product_tokens|category     |product     |\n","+---------------+--------------+-------------+------------+\n","|[grocery]      |[Oil]         | grocery     |Oil         |\n","|[grocery]      |[Oil]         | grocery     |Oil         |\n","|[Electronics]  |[Mobile]      |Electronics  |Mobile      |\n","|[electronics]  |[Mobile]      | electronics |Mobile      |\n","|[Fashion]      |[Jeans]       |Fashion      |Jeans       |\n","|[home]         |[Mixer]       | home        |Mixer       |\n","|[Fashion]      |[Shoes]       |Fashion      |Shoes       |\n","|[grocery]      |[Sugar]       | grocery     |Sugar       |\n","|[Electronics]  |[Mobile]      |Electronics  |Mobile      |\n","|[home]         |[Vacuum]      | home        |Vacuum      |\n","|[Home]         |[AirPurifier] |Home         |AirPurifier |\n","|[electronics]  |[Laptop]      | electronics |Laptop      |\n","|[grocery]      |[Sugar]       | grocery     |Sugar       |\n","|[Grocery]      |[Sugar]       |Grocery      |Sugar       |\n","|[home]         |[AirPurifier] | home        |AirPurifier |\n","|[Grocery]      |[Sugar]       |Grocery      |Sugar       |\n","|[electronics]  |[Laptop]      | electronics |Laptop      |\n","|[Electronics]  |[Tablet]      |Electronics  |Tablet      |\n","|[fashion]      |[Jeans]       | fashion     |Jeans       |\n","|[Home]         |[Mixer]       |Home         |Mixer       |\n","+---------------+--------------+-------------+------------+\n","\n","=== DQ Probe 7 — Sample suspicious rows (amount invalid/comma/non-numeric OR invalid_date) (CSV) ===\n","+-----------+-----------+---------+-----------+-----------+-------+------------+---------+\n","|order_id   |customer_id|city     |category   |product    |amount |order_date  |status   |\n","+-----------+-----------+---------+-----------+-----------+-------+------------+---------+\n","|ORD00013892|C013892    |Pune     |Fashion    |Shoes      |12,000 |2024-02-02  |Completed|\n","|ORD00034338|C034338    |Hyderabad|Electronics|Tablet     |55888  |invalid_date|Completed|\n","|ORD00278863|C028863    |Kolkata  |Home       |Mixer      |invalid|2024/02/13  |Completed|\n","|ORD00142646|C042646    |Chennai  |Fashion    |TShirt     |12,000 |2024-01-27  |Completed|\n","|ORD00055290|C005290    |Kolkata  |Electronics|Mobile     |invalid|invalid_date|Completed|\n","|ORD00266437|C016437    |Pune     |Electronics|Mobile     |invalid|2024-02-07  |Completed|\n","|ORD00187169|C037169    |Chennai  |Fashion    |Shoes      |invalid|2024-01-30  |Completed|\n","|ORD00056143|C006143    |Kolkata  |Grocery    |Sugar      |12,000 |2024-02-13  |Completed|\n","|ORD00276070|C026070    |Hyderabad|Electronics|Tablet     |invalid|2024-01-11  |Completed|\n","|ORD00103284|C003284    |Mumbai   |Electronics|Mobile     |invalid|2024-01-25  |Completed|\n","|ORD00130891|C030891    |Pune     |Fashion    |TShirt     |invalid|2024-02-01  |Completed|\n","|ORD00280744|C030744    |Chennai  |Fashion    |Shoes      |invalid|2024-01-05  |Completed|\n","|ORD00050830|C000830    |hyderabad|Fashion    |Jeans      |12,000 |2024/01/11  |Completed|\n","|ORD00121509|C021509    |Mumbai   |Home       |AirPurifier|12,000 |2024-01-10  |Completed|\n","|ORD00003933|C003933    |Bangalore|Home       |Mixer      |invalid|2024-02-03  |Completed|\n","|ORD00252776|C002776    |Chennai  |Electronics|Mobile     |invalid|2024-02-26  |Completed|\n","|ORD00135994|C035994    |Kolkata  |Electronics|Tablet     |53200  |invalid_date|Completed|\n","|ORD00067317|C017317    |Kolkata  |Home       |AirPurifier|invalid|2024-02-27  |Completed|\n","|ORD00204250|C004250    |Bangalore|Grocery    |Oil        |invalid|2024-01-11  |Completed|\n","|ORD00180367|C030367    |Kolkata  |Fashion    |TShirt     |invalid|08/01/2024  |Completed|\n","+-----------+-----------+---------+-----------+-----------+-------+------------+---------+\n","\n","\n","=== Phase 1 COMPLETE ===\n","Identified issues include (non-exhaustive):\n"," - Missing / blank values (amount, order_date, category, product, etc.)\n"," - 'amount' with 'invalid', commas ('12,000'), or non-numeric characters\n"," - Mixed date formats (YYYY-MM-DD, DD/MM/YYYY, YYYY/MM/DD) and 'invalid_date'\n"," - Leading/trailing whitespace & inconsistent casing in city/category/product\n"," - Occasional trailing spaces in tokens (e.g., 'Mobile ' or ' home ')\n","\n","Next Phase: define an explicit schema and validate rows against it.\n"]}]}]}