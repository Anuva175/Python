{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4RmDhE9rF/tzQW0oKxTe2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"feE7_B-DymoT","executionInfo":{"status":"ok","timestamp":1767610516699,"user_tz":-330,"elapsed":16833,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark=SparkSession.builder\\\n","    .appName(\"Day27\")\\\n","    .getOrCreate()"]},{"cell_type":"code","source":["\n","from google.colab import files\n","uploaded = files.upload()  # Select orders_raw.csv\n","list(uploaded.keys())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"hQdAC-DJ5o2K","executionInfo":{"status":"ok","timestamp":1767612319369,"user_tz":-330,"elapsed":47211,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"cc49de1e-e86d-4bd6-8e99-54128ed95dc9"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1003aa5e-9114-49ef-bd21-a09485970bc2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1003aa5e-9114-49ef-bd21-a09485970bc2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving orders_raw.csv to orders_raw.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["['orders_raw.csv']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["csv_path = \"/content/orders_raw.csv\""],"metadata":{"id":"HvcVUrni5zFV","executionInfo":{"status":"ok","timestamp":1767612361018,"user_tz":-330,"elapsed":42,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#Section 1\n","# 1) Ingestion\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession.builder\n","         .appName(\"Orders-Ingestion\")\n","         .config(\"spark.sql.session.timeZone\", \"UTC\")\n","         .getOrCreate())\n","\n","input_path = \"orders_raw.csv\"  # change to your path if needed\n","\n","df_raw = (spark.read\n","          .option(\"header\", True)        # first row is header\n","          .option(\"inferSchema\", False)  # read everything as strings\n","          .option(\"mode\", \"PERMISSIVE\")  # do not fail on malformed rows\n","          .csv(input_path))\n","\n","# Quick peek (optional)\n","df_raw.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNskIIKU6qqc","executionInfo":{"status":"ok","timestamp":1767612535588,"user_tz":-330,"elapsed":814,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"eb62ebbe-32c5-4902-c6af-88cd41d6aa6e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n","|order_id   |customer_id|city       |category   |product    |amount |order_date|status   |\n","+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n","|ORD00000000|C000000    | hyderabad | grocery   |Oil        |invalid|01/01/2024|Cancelled|\n","|ORD00000001|C000001    |Pune       |Grocery    |Sugar      |35430  |2024-01-02|Completed|\n","|ORD00000002|C000002    |Pune       |Electronics|Mobile     |65358  |2024-01-03|Completed|\n","|ORD00000003|C000003    |Bangalore  |Electronics|Laptop     |5558   |2024-01-04|Completed|\n","|ORD00000004|C000004    |Pune       |Home       |AirPurifier|33659  |2024-01-05|Completed|\n","+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n","only showing top 5 rows\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"vDYPyPV_6qYJ"}},{"cell_type":"code","source":["# 3) Print schema & total record count\n","\n","df_raw.printSchema()\n","\n","total_count = df_raw.count()\n","print(f\"Total records (raw): {total_count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EKVLqPR6uhI","executionInfo":{"status":"ok","timestamp":1767612570044,"user_tz":-330,"elapsed":597,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"4870167f-d7c3-4e49-bff9-17417f2ac7b1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- order_id: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- category: string (nullable = true)\n"," |-- product: string (nullable = true)\n"," |-- amount: string (nullable = true)\n"," |-- order_date: string (nullable = true)\n"," |-- status: string (nullable = true)\n","\n","Total records (raw): 300000\n"]}]},{"cell_type":"code","source":["# 4\n","from pyspark.sql.functions import col, trim\n","\n","df_trimmed = df_raw.select([trim(col(c)).alias(c) for c in df_raw.columns])"],"metadata":{"id":"HAYOqd5K65CH","executionInfo":{"status":"ok","timestamp":1767612625445,"user_tz":-330,"elapsed":66,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 5\n","from pyspark.sql.functions import lower, initcap\n","\n","df_std = (df_trimmed\n","          .withColumn(\"city_std\", initcap(lower(col(\"city\"))))\n","          .withColumn(\"category_std\", lower(col(\"category\")))\n","          .withColumn(\"product_std\", lower(col(\"product\"))))\n","\n","\n","df_std.select(\"city\", \"city_std\", \"category\", \"category_std\", \"product\", \"product_std\").show(10, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSu3QfGJ7ENU","executionInfo":{"status":"ok","timestamp":1767612658535,"user_tz":-330,"elapsed":2851,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"024b6702-bafe-4dc3-e6c5-5132e1898a40"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---------+-----------+------------+-----------+-----------+\n","|city     |city_std |category   |category_std|product    |product_std|\n","+---------+---------+-----------+------------+-----------+-----------+\n","|hyderabad|Hyderabad|grocery    |grocery     |Oil        |oil        |\n","|Pune     |Pune     |Grocery    |grocery     |Sugar      |sugar      |\n","|Pune     |Pune     |Electronics|electronics |Mobile     |mobile     |\n","|Bangalore|Bangalore|Electronics|electronics |Laptop     |laptop     |\n","|Pune     |Pune     |Home       |home        |AirPurifier|airpurifier|\n","|Delhi    |Delhi    |Fashion    |fashion     |Jeans      |jeans      |\n","|Delhi    |Delhi    |Grocery    |grocery     |Sugar      |sugar      |\n","|Pune     |Pune     |Grocery    |grocery     |Rice       |rice       |\n","|Bangalore|Bangalore|Fashion    |fashion     |Jeans      |jeans      |\n","|Kolkata  |Kolkata  |Electronics|electronics |Laptop     |laptop     |\n","+---------+---------+-----------+------------+-----------+-----------+\n","only showing top 10 rows\n"]}]},{"cell_type":"code","source":["#6\n","from pyspark.sql.functions import regexp_replace, when\n","\n","df_amt = (df_std\n","          .withColumn(\"amount_clean\", regexp_replace(col(\"amount\"), r\"[^0-9]\", \"\"))\n","\n","          .withColumn(\"amount_int\", when(col(\"amount_clean\") == \"\", None).otherwise(col(\"amount_clean\").cast(\"int\"))))\n","\n","df_amt.select(\"amount\", \"amount_clean\", \"amount_int\").show(15, truncate=False)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sL6y03Ws7MRG","executionInfo":{"status":"ok","timestamp":1767612737417,"user_tz":-330,"elapsed":327,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"7626d369-8818-4a1a-8184-d4652bef99c2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------+----------+\n","|amount |amount_clean|amount_int|\n","+-------+------------+----------+\n","|invalid|            |NULL      |\n","|35430  |35430       |35430     |\n","|65358  |65358       |65358     |\n","|5558   |5558        |5558      |\n","|33659  |33659       |33659     |\n","|8521   |8521        |8521      |\n","|42383  |42383       |42383     |\n","|45362  |45362       |45362     |\n","|10563  |10563       |10563     |\n","|63715  |63715       |63715     |\n","|66576  |66576       |66576     |\n","|50318  |50318       |50318     |\n","|84768  |84768       |84768     |\n","|79121  |79121       |79121     |\n","|79469  |79469       |79469     |\n","+-------+------------+----------+\n","only showing top 15 rows\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, coalesce, col, trim\n","\n","df_tmp = df_raw.withColumn(\"order_date\", trim(col(\"order_date\")))\n","\n","formats = [\n","    \"yyyy-MM-dd\",      # 2024-12-31\n","    \"MM/dd/yyyy\",      # 12/31/2024\n","    \"dd/MM/yyyy\",      # 31/12/2024\n","    \"dd-MMM-yyyy\",     # 31-Dec-2024\n","    \"yyyy/MM/dd\",      # 2024/12/31\n","]\n","\n","parsed_exprs = [to_date(col(\"order_date\"), f) for f in formats]\n","\n","df_parsed = df_tmp.withColumn(\"order_date_parsed\", coalesce(*parsed_exprs))\n","\n","df_parsed.select(\"order_id\", \"order_date\", \"order_date_parsed\").show(10, truncate=False)\n","df_parsed.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":636},"id":"yuWubIdq8YZn","executionInfo":{"status":"error","timestamp":1767612981849,"user_tz":-330,"elapsed":365,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"7b9da773-b2f2-403e-ca12-c364f9eb2865"},"execution_count":21,"outputs":[{"output_type":"error","ename":"DateTimeException","evalue":"[CANNOT_PARSE_TIMESTAMP] Text '01/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDateTimeException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3415670636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_date_parsed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparsed_exprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"order_date_parsed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdf_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 )\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDateTimeException\u001b[0m: [CANNOT_PARSE_TIMESTAMP] Text '01/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"]}]},{"cell_type":"code","source":["#9\n","df_dedup = df_parsed.dropDuplicates([\"order_id\"])\n","count_after_dedup = df_dedup.count()\n","print(f\"Total records AFTER dedup on order_id: {count_after_dedup}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lotcSFWj7jU_","executionInfo":{"status":"ok","timestamp":1767612992650,"user_tz":-330,"elapsed":3537,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"9451935a-2781-45b3-f121-0b034b1d425a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Total records AFTER dedup on order_id: 300000\n"]}]},{"cell_type":"code","source":["#10\n","df_completed = df_dedup.filter(lower(trim(col(\"status\"))) == \"completed\")\n","count_after_filter = df_completed.count()\n","print(f\"Total records AFTER filtering status=Completed: {count_after_filter}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WF8RvRe8dxP","executionInfo":{"status":"ok","timestamp":1767613153311,"user_tz":-330,"elapsed":5438,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"bd2f3250-2eae-4dd9-d6b3-a9a63088516a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Total records AFTER filtering status=Completed: 285000\n"]}]},{"cell_type":"code","source":["#11\n","count_before = df_parsed.count()\n","print(f\"Total records BEFORE dedup/filter: {count_before}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrL092Ml9F7q","executionInfo":{"status":"ok","timestamp":1767613189837,"user_tz":-330,"elapsed":345,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"8407873b-b5f9-481c-b8d9-eaa7ec76476f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Total records BEFORE dedup/filter: 300000\n"]}]},{"cell_type":"code","source":["#13\n","from pyspark.sql.functions import col, countDistinct\n","\n","plan_df = (\n","    df_parsed\n","      .dropDuplicates([\"order_id\"])\n","      .filter(col(\"status\").isNotNull())\n","      .groupBy(\"city\")\n","      .agg(countDistinct(\"order_id\").alias(\"orders\"))\n",")\n","\n","\n","plan_df.explain(True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBl45JPa9Nm9","executionInfo":{"status":"ok","timestamp":1767613242617,"user_tz":-330,"elapsed":131,"user":{"displayName":"Anuvarsini A","userId":"11249603367073252026"}},"outputId":"aaddc8e9-1dff-445c-99b1-572230749300"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["== Parsed Logical Plan ==\n","'Aggregate ['city], ['city, 'count(distinct 'order_id) AS orders#361]\n","+- Filter isnotnull(status#168)\n","   +- Deduplicate [order_id#161]\n","      +- Project [order_id#161, customer_id#162, city#163, category#164, product#165, amount#166, order_date#261, status#168, coalesce(to_date(order_date#261, Some(yyyy-MM-dd), Some(UTC), true), to_date(order_date#261, Some(MM/dd/yyyy), Some(UTC), true), to_date(order_date#261, Some(dd/MM/yyyy), Some(UTC), true), to_date(order_date#261, Some(dd-MMM-yyyy), Some(UTC), true), to_date(order_date#261, Some(yyyy/MM/dd), Some(UTC), true)) AS order_date_parsed#262]\n","         +- Project [order_id#161, customer_id#162, city#163, category#164, product#165, amount#166, trim(order_date#167, None) AS order_date#261, status#168]\n","            +- Relation [order_id#161,customer_id#162,city#163,category#164,product#165,amount#166,order_date#167,status#168] csv\n","\n","== Analyzed Logical Plan ==\n","city: string, orders: bigint\n","Aggregate [city#163], [city#163, count(distinct order_id#161) AS orders#361L]\n","+- Filter isnotnull(status#168)\n","   +- Deduplicate [order_id#161]\n","      +- Project [order_id#161, customer_id#162, city#163, category#164, product#165, amount#166, order_date#261, status#168, coalesce(to_date(order_date#261, Some(yyyy-MM-dd), Some(UTC), true), to_date(order_date#261, Some(MM/dd/yyyy), Some(UTC), true), to_date(order_date#261, Some(dd/MM/yyyy), Some(UTC), true), to_date(order_date#261, Some(dd-MMM-yyyy), Some(UTC), true), to_date(order_date#261, Some(yyyy/MM/dd), Some(UTC), true)) AS order_date_parsed#262]\n","         +- Project [order_id#161, customer_id#162, city#163, category#164, product#165, amount#166, trim(order_date#167, None) AS order_date#261, status#168]\n","            +- Relation [order_id#161,customer_id#162,city#163,category#164,product#165,amount#166,order_date#167,status#168] csv\n","\n","== Optimized Logical Plan ==\n","Aggregate [city#375], [city#375, count(distinct order_id#161) AS orders#361L]\n","+- Project [order_id#161, city#375]\n","   +- Filter isnotnull(status#385)\n","      +- Aggregate [order_id#161], [order_id#161, first(city#163, false) AS city#375, first(status#168, false) AS status#385]\n","         +- Project [order_id#161, city#163, status#168]\n","            +- Relation [order_id#161,customer_id#162,city#163,category#164,product#165,amount#166,order_date#167,status#168] csv\n","\n","== Physical Plan ==\n","AdaptiveSparkPlan isFinalPlan=false\n","+- HashAggregate(keys=[city#375], functions=[count(distinct order_id#161)], output=[city#375, orders#361L])\n","   +- Exchange hashpartitioning(city#375, 200), ENSURE_REQUIREMENTS, [plan_id=579]\n","      +- HashAggregate(keys=[city#375], functions=[partial_count(distinct order_id#161)], output=[city#375, count#390L])\n","         +- HashAggregate(keys=[city#375, order_id#161], functions=[], output=[city#375, order_id#161])\n","            +- HashAggregate(keys=[city#375, order_id#161], functions=[], output=[city#375, order_id#161])\n","               +- Project [order_id#161, city#375]\n","                  +- Filter isnotnull(status#385)\n","                     +- SortAggregate(key=[order_id#161], functions=[first(city#163, false), first(status#168, false)], output=[order_id#161, city#375, status#385])\n","                        +- Sort [order_id#161 ASC NULLS FIRST], false, 0\n","                           +- Exchange hashpartitioning(order_id#161, 200), ENSURE_REQUIREMENTS, [plan_id=570]\n","                              +- SortAggregate(key=[order_id#161], functions=[partial_first(city#163, false), partial_first(status#168, false)], output=[order_id#161, first#396, valueSet#397, first#398, valueSet#399])\n","                                 +- Sort [order_id#161 ASC NULLS FIRST], false, 0\n","                                    +- FileScan csv [order_id#161,city#163,status#168] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders_raw.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,status:string>\n","\n"]}]},{"cell_type":"code","source":["#14\n","from pyspark.sql.functions import to_date, coalesce, col, trim\n","\n","df_tmp = df_raw.withColumn(\"order_date\", trim(col(\"order_date\")))\n","\n","formats = [\n","    \"yyyy-MM-dd\",      # 2024-12-31\n","    \"MM/dd/yyyy\",      # 12/31/2024\n","    \"dd/MM/yyyy\",      # 31/12/2024\n","    \"dd-MMM-yyyy\",     # 31-Dec-2024\n","    \"yyyy/MM/dd\",      # 2024/12/31\n","]\n","\n","parsed_exprs = [to_date(col(\"order_date\"), f) for f in formats]\n","\n","df_parsed = df_tmp.withColumn(\"order_date_parsed\", coalesce(*parsed_exprs))\n","\n","df_parsed.select(\"order_id\", \"order_date\", \"order_date_parsed\").show(10, truncate=False)\n","df_parsed.printSchema()\n",""],"metadata":{"id":"zVmspYA29bMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#15\n","completed_df.cache(); completed_df.count()  # materialize"],"metadata":{"id":"XxnKiB229hkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#16\n","\n","rev_city_df = (completed_df\n","               .groupBy(\"city\")\n","               .agg(F.sum(\"amount_int\").alias(\"total_revenue\"))\n","               .orderBy(F.desc(\"total_revenue\")))\n","rev_city_df.show(10, truncate=False)\n"],"metadata":{"id":"HmqBV_1T_Vb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#17\n","\n","rev_category_df = (completed_df\n","                   .groupBy(\"category\")\n","                   .agg(F.sum(\"amount_int\").alias(\"total_revenue\"))\n","                   .orderBy(F.desc(\"total_revenue\")))\n","rev_category_df.show(10, truncate=False)\n"],"metadata":{"id":"OzKHbN5o_ZjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#18\n","\n","top_products_df = (completed_df\n","                   .groupBy(\"product\")\n","                   .agg(F.sum(\"amount_int\").alias(\"total_revenue\"))\n","                   .orderBy(F.desc(\"total_revenue\"))\n","                   .limit(5))\n","top_products_df.show(truncate=False)\n"],"metadata":{"id":"TQpe1yrW_dPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#19\n","\n","aov_city_df = (completed_df\n","               .groupBy(\"city\")\n","               .agg(F.avg(\"amount_int\").alias(\"avg_order_value\"))\n","               .orderBy(F.desc(\"avg_order_value\")))\n","aov_city_df.show(10, truncate=False)\n"],"metadata":{"id":"vQWRP6_-_gdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#20\n","\n","from pyspark.sql.window import Window\n","\n","city_rank_df = rev_city_df.withColumn(\n","    \"rank_by_revenue\",\n","    F.dense_rank().over(Window.orderBy(F.desc(\"total_revenue\")))\n",")\n","city_rank_df.show(10, truncate=False)\n"],"metadata":{"id":"1Ij_WGAY_jFj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#21\n","\n","prod_cat_rev_df = (completed_df\n","                   .groupBy(\"category\",\"product\")\n","                   .agg(F.sum(\"amount_int\").alias(\"total_revenue\")))\n","\n","w_cat = Window.partitionBy(\"category\").orderBy(F.desc(\"total_revenue\"))\n","prod_cat_rank_df = prod_cat_rev_df.withColumn(\"rank_in_category\", F.dense_rank().over(w_cat))\n","prod_cat_rank_df.orderBy(\"category\",\"rank_in_category\").show(20, truncate=False)\n"],"metadata":{"id":"Dm1wXBio_nd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#22\n","top_prod_per_cat_df = prod_cat_rank_df.filter(F.col(\"rank_in_category\") == 1)\n","top_prod_per_cat_df.orderBy(\"category\").show(truncate=False)\n"],"metadata":{"id":"0q6Bant2_rj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#23\n","\n","(clean_df\n"," .filter(F.col(\"status\") == \"Completed\")  # optional\n"," .select(\"order_id\",\"customer_id\",\"city\",\"category\",\"product\",\"amount_int\",\"order_dt\",\"status\")\n"," .write.mode(\"overwrite\"))\n"],"metadata":{"id":"RvACfHhV_vQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#24\n","\n","rev_city_df.write.mode(\"overwrite\").orc(\"/path/to/outputs/rev_city_orc\")\n","rev_category_df.write.mode(\"overwrite\").orc(\"/path/to/outputs/rev_category_orc\")\n","top_products_df.write.mode(\"overwrite\").orc(\"/path/to/outputs/top_products_orc\")\n","aov_city_df.write.mode(\"overwrite\").orc(\"/path/to/outputs/aov_city_orc\")\n"],"metadata":{"id":"cbuwfSDD_2y2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#26\n","\n","df_filtered = df.filter(F.col(\"amount_int\") > 50000)\n","df_filtered.show()\n"],"metadata":{"id":"Izl8UZhV_6DG"},"execution_count":null,"outputs":[]}]}